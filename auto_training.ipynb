{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_source = \"/home/hassaan/Downloads/rgb_split/\"\n",
    "depth_source = \"/home/hassaan/Downloads/dataset/images/\"\n",
    "labels_destination = \"/home/hassaan/Downloads/dataset/labels/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.png', '1.png']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_list = os.listdir(rgb_source)\n",
    "rgb_list.sort(key=lambda x: int(x.split(\".\")[0]))\n",
    "rgb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m-pose.pt to 'models/yolov8m-pose.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50.8M/50.8M [00:03<00:00, 16.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m.pt to 'models/yolov8m.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49.7M/49.7M [00:01<00:00, 27.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "pose_model = YOLO(\"models/yolov8m-pose.pt\")\n",
    "object_model = YOLO(\"models/yolov8m.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_object_detection(frame, height, width):\n",
    "\n",
    "    bbox_results = object_model(frame)\n",
    "    bboxes = []\n",
    "    # Render the results\n",
    "    for result in bbox_results:\n",
    "        if result.boxes is not None:\n",
    "            for box in result.boxes:\n",
    "                label = object_model.names[int(box.cls[0])]\n",
    "                confidence = box.conf[0]\n",
    "                if label == \"person\" and confidence > 0.4:\n",
    "                    bbox = box.xyxy[0].numpy().astype(int)\n",
    "\n",
    "                    bboxes.append(bbox)\n",
    "                    x1, y1, x2, y2 = bbox\n",
    "                    while False:\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                        cv2.putText(\n",
    "                            frame,\n",
    "                            f\"{label} {confidence:.2f}\",\n",
    "                            (x1, y1 - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.5,\n",
    "                            (0, 255, 0),\n",
    "                            2,\n",
    "                        )\n",
    "                    x_center, y_center, box_width, box_height = (\n",
    "                        (x1 + x2) / 2,\n",
    "                        (y1 + y2) / 2,\n",
    "                        x1 + x2,\n",
    "                        y1 + y2,\n",
    "                    )\n",
    "                    return [\n",
    "                        x_center / width,\n",
    "                        y_center / height,\n",
    "                        box_width / width,\n",
    "                        box_height / height,\n",
    "                    ], frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_annotations(path):\n",
    "    frame = cv2.imread(path)\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    bbox_results, frame = test_object_detection(frame, height, width)\n",
    "\n",
    "    pose_results = pose_model.predict(frame)\n",
    "    keypoints = pose_results[0].keypoints\n",
    "    if len(keypoints) > 0:\n",
    "        keypoints = keypoints[0].cpu().numpy()\n",
    "    else:\n",
    "        keypoints = None\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    annotations = [0] + bbox_results\n",
    "    if keypoints is not None:\n",
    "        xy = keypoints.data[0]\n",
    "        conf = keypoints.conf[0]\n",
    "        for i in range(len(xy)):\n",
    "            x, y, conf = xy[i]\n",
    "            if conf > 0:\n",
    "                cv2.circle(frame, (int(x), int(y)), 2, (0, 255, 0), -1)\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f\"{i}-{int(conf*100)}\",\n",
    "                    (int(x), int(y) - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (255, 0, 0),\n",
    "                    1,\n",
    "                )\n",
    "            annotations += [x / width, y / height, conf]\n",
    "    cv2.imwrite(\"./output.png\", frame)\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 544x640 1 person, 1 tie, 2351.1ms\n",
      "Speed: 9.2ms preprocess, 2351.1ms inference, 206.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "\n",
      "0: 544x640 1 person, 2125.6ms\n",
      "Speed: 4.5ms preprocess, 2125.6ms inference, 6.0ms postprocess per image at shape (1, 3, 544, 640)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0.49362244897959184,\n",
       " 0.5182926829268293,\n",
       " 0.9872448979591837,\n",
       " 1.0365853658536586,\n",
       " 0.45090138182348133,\n",
       " 0.3075299146698742,\n",
       " 0.99155056,\n",
       " 0.47692143187230945,\n",
       " 0.24344851331013004,\n",
       " 0.98072803,\n",
       " 0.3964913815868144,\n",
       " 0.26409858610571885,\n",
       " 0.9802058,\n",
       " 0.5214869440818319,\n",
       " 0.2500436131547137,\n",
       " 0.859938,\n",
       " 0.3270313496492347,\n",
       " 0.3048902604638076,\n",
       " 0.88930154,\n",
       " 0.6835650229940609,\n",
       " 0.5639311627643865,\n",
       " 0.98636,\n",
       " 0.2443479226560009,\n",
       " 0.6433977731844274,\n",
       " 0.9712768,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.19218221,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.108061604,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.06869289,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.04261578,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0082016215,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.006917274,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0019635519,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0016172073,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.00062964304,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0005830445]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/home/hassaan/Downloads/SCREENSHOTS/nightCrawler.jpg\"\n",
    "human_annotations(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x416 1 person, 1 potted plant, 1754.1ms\n",
      "Speed: 3.4ms preprocess, 1754.1ms inference, 23.9ms postprocess per image at shape (1, 3, 640, 416)\n",
      "\n",
      "0: 640x416 1 person, 1626.4ms\n",
      "Speed: 7.7ms preprocess, 1626.4ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 416)\n",
      "saved file : /home/hassaan/Downloads/dataset/labels/0.txt\n",
      "\n",
      "0: 576x640 1 person, 2 handbags, 3 chairs, 2011.6ms\n",
      "Speed: 5.6ms preprocess, 2011.6ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "\n",
      "0: 576x640 2 persons, 2034.1ms\n",
      "Speed: 7.0ms preprocess, 2034.1ms inference, 2.6ms postprocess per image at shape (1, 3, 576, 640)\n",
      "saved file : /home/hassaan/Downloads/dataset/labels/1.txt\n"
     ]
    }
   ],
   "source": [
    "for image in rgb_list:\n",
    "    path = rgb_source + image\n",
    "    number = int(image.split(\".\")[0])\n",
    "\n",
    "    text = human_annotations(path)\n",
    "    text = str(text).lstrip(\"[\").rstrip(\"]\").replace(\",\", \"\")\n",
    "\n",
    "    with open(\n",
    "        f\"{labels_destination}/{number}.txt\",\n",
    "        \"w+\",\n",
    "    ) as file:\n",
    "        file.write(text)\n",
    "        print(f\"saved file : {labels_destination}{number}.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0 0.09795673076923077 0.4361111111111111 0.19591346153846154 0.8722222222222222 0.0 0.0 0.42677712 0.0 0.0 0.1960839 0.0 0.0 0.27187887 0.0 0.0 0.4001799 0.07273994500820453 0.21386241912841797 0.6183357 0.03756919961709242 0.28499234517415367 0.91871655 0.1186533707838792 0.2850828594631619 0.9771234 0.0034462629029384027 0.39101732042100695 0.670641 0.1671757147862361 0.3979143778483073 0.9120515 0.0 0.0 0.4546735 0.1345757246017456 0.37044330173068574 0.7553778 0.07511217319048367 0.45458780924479164 0.9609252 0.13105473151573768 0.45579274495442706 0.9788629 0.10324607445643498 0.5775990804036458 0.9355841 0.16154109514676607 0.5598100026448568 0.96521175 0.11838032649113582 0.682875484890408 0.79988086 0.18702829801119292 0.6609213087293837 0.8649284'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
