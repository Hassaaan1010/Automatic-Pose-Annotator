{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from utils.bbox_area import bbox_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"models/yolov8m.pt\")\n",
    "# model in multi_people_splitting/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_object_detection(path, show=False):\n",
    "    # frame = cv2.imread(path)[:, 300:]\n",
    "    frame = cv2.imread(path)\n",
    "\n",
    "    results = model(frame)\n",
    "    bboxes = []\n",
    "    # Render the results\n",
    "    for result in results:\n",
    "        if result.boxes is not None:\n",
    "            for box in result.boxes:\n",
    "                label = model.names[int(box.cls[0])]\n",
    "                confidence = box.conf[0]\n",
    "                if label == \"person\" and confidence > 0:\n",
    "                    bbox = box.xyxy[0].numpy().astype(int)\n",
    "                    # print(bbox)\n",
    "                    if len(bboxes) == 4:\n",
    "                        break\n",
    "                    if bbox_area(bbox) < 40000:\n",
    "                        continue\n",
    "                    bboxes.append(bbox)\n",
    "                    x1, y1, x2, y2 = bbox\n",
    "\n",
    "                    # Draw bounding box and label\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    cv2.putText(\n",
    "                        frame,\n",
    "                        f\"{label} {confidence:.2f}\",\n",
    "                        (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.5,\n",
    "                        (0, 255, 0),\n",
    "                        2,\n",
    "                    )\n",
    "    while show:\n",
    "        cv2.imshow(\"C\", frame)\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "    return frame, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/hassaan/Downloads/train_2500/images_final/285.png\"\n",
    "# path = \"/home/hassaan/Downloads/allFinal/091363275.jpg\"\n",
    "# path = \"/home/hassaan/Downloads/allFinal/111.png\"\n",
    "# path = \"/home/hassaan/Downloads/PoseDetection/folder1/content/hosp11.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 384x640 5 persons, 968.9ms\n",
      "Speed: 4.1ms preprocess, 968.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "image, bboxes = test_object_detection(path, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 976,  173, 1114,  519]), array([ 96, 142, 245, 440])]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[535 143 633 361]\n",
      "[416 157 489 349]\n",
      "[362 119 438 337]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bboxes)):\n",
    "    print(bboxes[i])\n",
    "    x1, y1, x2, y2 = bboxes[i]\n",
    "    cropedImage = image[y1:y2, x1:x2]\n",
    "    while True:\n",
    "        cv2.imshow(f\"{i} image: \", cropedImage)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "approved = 0\n",
    "disapproved = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Display the frame\n",
    "    h, w, _ = image.shape\n",
    "    # print(h, w)\n",
    "    # image = cv2.resize(image, (321 * 3, 491 * 3))\n",
    "    cv2.imshow(\"yeich\", image)\n",
    "    # plt.imshow(img)\n",
    "    # print(\"worked\")\n",
    "    cv2.imwrite(\"/home/hassaan/Downloads/asdfDepth.png\", image)\n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "\n",
    "    fullPath = path + str(i) + \".png\"\n",
    "    print(fullPath)\n",
    "    image = test_object_detection(fullPath)\n",
    "\n",
    "    for bbox in bboxs:\n",
    "        \n",
    "        image = image[x1:,:]\n",
    "\n",
    "    while True:\n",
    "        # Display the frame\n",
    "        h, w, _ = image.shape\n",
    "        # print(h, w)\n",
    "        # image = cv2.resize(image, (321 * 3, 491 * 3))\n",
    "        cv2.imshow(\"YOLOv8 Nano\", image)\n",
    "        # plt.imshow(img)\n",
    "        # print(\"worked\")\n",
    "        cv2.imwrite(\"/home/hassaan/Downloads/asdfDepth.png\", image)\n",
    "        # Exit on 'q' key press\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"q\"):\n",
    "            approved += 1\n",
    "            break\n",
    "        if cv2.waitKey(10) & 0xFF == ord(\"p\"):\n",
    "            disapproved += 1\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "print(\"approved :\", approved)\n",
    "print(\"disapproved :\", disapproved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 person, 2578.5ms\n",
      "Speed: 126.4ms preprocess, 2578.5ms inference, 16.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[799 100 885 309]\n",
      "\n",
      "0: 480x640 1 person, 1643.2ms\n",
      "Speed: 7.4ms preprocess, 1643.2ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "[492 160 723 439]\n",
      "\n",
      "0: 384x640 3 persons, 1140.2ms\n",
      "Speed: 5.6ms preprocess, 1140.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[ 763    2 1280  705]\n",
      "[265 251 417 503]\n",
      "\n",
      "0: 384x640 3 persons, 1175.2ms\n",
      "Speed: 8.0ms preprocess, 1175.2ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[ 875  289 1022  613]\n",
      "[  0   5 470 706]\n",
      "[526 218 788 710]\n",
      "\n",
      "0: 416x640 1 person, 1314.1ms\n",
      "Speed: 5.4ms preprocess, 1314.1ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1279.3ms\n",
      "Speed: 4.6ms preprocess, 1279.3ms inference, 12.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[511 192 563 414]\n",
      "\n",
      "0: 384x640 1 person, 1122.3ms\n",
      "Speed: 7.0ms preprocess, 1122.3ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1170.3ms\n",
      "Speed: 5.2ms preprocess, 1170.3ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1147.4ms\n",
      "Speed: 4.0ms preprocess, 1147.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1116    0 1280  710]\n",
      "\n",
      "0: 352x640 1 person, 1249.2ms\n",
      "Speed: 13.5ms preprocess, 1249.2ms inference, 2.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "[133  41 234 296]\n",
      "\n",
      "0: 352x640 (no detections), 1018.2ms\n",
      "Speed: 3.5ms preprocess, 1018.2ms inference, 0.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 1 person, 1140.6ms\n",
      "Speed: 3.5ms preprocess, 1140.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[379  96 582 541]\n",
      "\n",
      "0: 384x640 1 person, 1180.1ms\n",
      "Speed: 4.9ms preprocess, 1180.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[ 901  224 1081  596]\n",
      "\n",
      "0: 384x640 2 persons, 1141.0ms\n",
      "Speed: 7.1ms preprocess, 1141.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[690 359 824 695]\n",
      "\n",
      "0: 384x640 2 persons, 1414.8ms\n",
      "Speed: 4.8ms preprocess, 1414.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[488 147 585 360]\n",
      "\n",
      "0: 384x640 2 persons, 1187.2ms\n",
      "Speed: 5.0ms preprocess, 1187.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[554  75 655 353]\n",
      "\n",
      "0: 384x640 2 persons, 1186.4ms\n",
      "Speed: 5.5ms preprocess, 1186.4ms inference, 5.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[  0   8 413 703]\n",
      "[724  76 873 455]\n",
      "\n",
      "0: 384x640 1 person, 1370.6ms\n",
      "Speed: 3.5ms preprocess, 1370.6ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[475 118 850 713]\n",
      "\n",
      "0: 352x640 (no detections), 1038.8ms\n",
      "Speed: 5.4ms preprocess, 1038.8ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1198.6ms\n",
      "Speed: 3.9ms preprocess, 1198.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[464 142 575 371]\n",
      "\n",
      "0: 384x640 3 persons, 1173.1ms\n",
      "Speed: 3.9ms preprocess, 1173.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[277  56 441 382]\n",
      "[712  13 854 472]\n",
      "\n",
      "0: 384x640 1 person, 1148.5ms\n",
      "Speed: 5.1ms preprocess, 1148.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1195.5ms\n",
      "Speed: 4.7ms preprocess, 1195.5ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[   0   22  452 1063]\n",
      "\n",
      "0: 384x640 2 persons, 1183.0ms\n",
      "Speed: 3.4ms preprocess, 1183.0ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[501 168 686 550]\n",
      "[726 339 965 713]\n",
      "\n",
      "0: 384x640 3 persons, 1170.3ms\n",
      "Speed: 7.0ms preprocess, 1170.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[327 130 468 397]\n",
      "[157 220 230 419]\n",
      "[219 260 387 673]\n",
      "\n",
      "0: 384x640 1 person, 1186.5ms\n",
      "Speed: 6.0ms preprocess, 1186.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[ 883  127 1000  448]\n",
      "\n",
      "0: 384x640 1 person, 1251.5ms\n",
      "Speed: 4.0ms preprocess, 1251.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[564 172 738 462]\n",
      "\n",
      "0: 384x640 1 person, 1184.2ms\n",
      "Speed: 4.8ms preprocess, 1184.2ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[ 86 508 160 678]\n",
      "\n",
      "0: 384x640 3 persons, 1261.4ms\n",
      "Speed: 5.7ms preprocess, 1261.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[172 219 478 638]\n",
      "[430  97 617 529]\n",
      "\n",
      "0: 384x640 (no detections), 1228.5ms\n",
      "Speed: 8.2ms preprocess, 1228.5ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1182.1ms\n",
      "Speed: 5.0ms preprocess, 1182.1ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 1361.6ms\n",
      "Speed: 5.2ms preprocess, 1361.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1365.6ms\n",
      "Speed: 3.6ms preprocess, 1365.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[575 199 708 457]\n",
      "\n",
      "0: 384x640 1 person, 1461.7ms\n",
      "Speed: 18.0ms preprocess, 1461.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[339  66 518 512]\n",
      "\n",
      "0: 384x640 1 person, 1147.9ms\n",
      "Speed: 5.4ms preprocess, 1147.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[373 160 591 669]\n",
      "\n",
      "0: 384x640 1 person, 1143.6ms\n",
      "Speed: 5.6ms preprocess, 1143.6ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[229 423 446 711]\n",
      "\n",
      "0: 384x640 2 persons, 1323.8ms\n",
      "Speed: 3.1ms preprocess, 1323.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1073   71 1224  443]\n",
      "\n",
      "0: 384x640 (no detections), 1236.8ms\n",
      "Speed: 4.8ms preprocess, 1236.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1284.9ms\n",
      "Speed: 3.5ms preprocess, 1284.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[566  31 712 181]\n",
      "[369  38 808 706]\n",
      "[782 172 985 605]\n",
      "\n",
      "0: 384x640 1 person, 1233.8ms\n",
      "Speed: 3.6ms preprocess, 1233.8ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[ 899  357 1279  711]\n",
      "\n",
      "0: 384x640 3 persons, 1248.9ms\n",
      "Speed: 6.9ms preprocess, 1248.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[686   6 979 709]\n",
      "\n",
      "0: 384x640 1 person, 1237.0ms\n",
      "Speed: 3.5ms preprocess, 1237.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[533 324 693 506]\n",
      "\n",
      "0: 384x640 1 person, 1391.5ms\n",
      "Speed: 5.0ms preprocess, 1391.5ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[297  64 490 316]\n",
      "\n",
      "0: 384x640 1 person, 1314.4ms\n",
      "Speed: 10.9ms preprocess, 1314.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[ 885    2 1280  708]\n",
      "\n",
      "0: 384x640 (no detections), 1245.9ms\n",
      "Speed: 3.6ms preprocess, 1245.9ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1265.0ms\n",
      "Speed: 7.4ms preprocess, 1265.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[741 260 881 472]\n",
      "\n",
      "0: 384x640 1 person, 1344.0ms\n",
      "Speed: 4.4ms preprocess, 1344.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[1066  306 1167  489]\n",
      "\n",
      "0: 384x640 1 person, 1354.7ms\n",
      "Speed: 7.4ms preprocess, 1354.7ms inference, 6.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[ 968  196 1151  512]\n",
      "\n",
      "0: 384x640 3 persons, 1338.1ms\n",
      "Speed: 7.2ms preprocess, 1338.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[776 317 961 655]\n",
      "[ 859    1 1280  707]\n",
      "\n",
      "0: 384x640 2 persons, 1317.6ms\n",
      "Speed: 6.3ms preprocess, 1317.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[698  75 974 578]\n",
      "[386   9 768 607]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50\u001b[39m):\n\u001b[1;32m      2\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/hassaan/Downloads/DPT2/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtest_object_detection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 30\u001b[0m, in \u001b[0;36mtest_object_detection\u001b[0;34m(path, show)\u001b[0m\n\u001b[1;32m     20\u001b[0m                 cv2\u001b[38;5;241m.\u001b[39mputText(\n\u001b[1;32m     21\u001b[0m                     frame,\n\u001b[1;32m     22\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidence\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m                     \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     28\u001b[0m                 )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m show:\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     32\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(0, 50):\n",
    "    path = f\"/home/hassaan/Downloads/DPT2/{i}.png\"\n",
    "\n",
    "    test_object_detection(path, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
